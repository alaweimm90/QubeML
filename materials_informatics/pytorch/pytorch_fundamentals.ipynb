{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_cell"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# üì¶ Install PyTorch (Colab only)\n",
        "# ==========================\n",
        "!pip install torch torchvision torch-geometric matplotlib numpy pandas scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_cell"
      },
      "source": [
        "# Deep Learning with PyTorch: A Beginner's Guide\n",
        "\n",
        "This notebook introduces fundamental concepts in deep learning using PyTorch, specifically for materials science applications. We will explore tensor operations, neural network architectures, crystal descriptors, property prediction, graph neural networks, and real materials applications.\n",
        "\n",
        "**Learning Path**: Tensors & Autograd ‚Üí Neural Networks ‚Üí Crystal Descriptors ‚Üí Bandgap Prediction ‚Üí Graph Neural Networks ‚Üí Materials Applications\n",
        "\n",
        "Let's start by setting up our deep learning environment for materials science."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_cell"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for deep learning and materials science\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(\"üéØ Deep learning environment ready for materials science!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concept_1_theory"
      },
      "source": [
        "# ‚úÖ 1. Tensors & Autograd - Deep Learning Foundations\n",
        "\n",
        "PyTorch tensors are the fundamental data structure for deep learning, similar to NumPy arrays but with additional capabilities for automatic differentiation (autograd). Understanding tensors and gradients is essential for materials property prediction.\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "A tensor is a generalization of vectors and matrices to higher dimensions:\n",
        "- **Scalar** (0D): $s$\n",
        "- **Vector** (1D): $\\mathbf{v} = [v_1, v_2, ..., v_n]$\n",
        "- **Matrix** (2D): $\\mathbf{M} = \\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix}$\n",
        "- **Tensor** (nD): Multidimensional array\n",
        "\n",
        "**Automatic differentiation** computes gradients via chain rule:\n",
        "$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$\n",
        "\n",
        "## Applications in Materials Science\n",
        "\n",
        "Tensors represent:\n",
        "- **Crystal structures**: Atomic positions and lattice parameters\n",
        "- **Material properties**: Bandgaps, formation energies, elastic constants\n",
        "- **Feature vectors**: Descriptors for machine learning models\n",
        "\n",
        "This section demonstrates tensor operations essential for materials informatics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "concept_1_code"
      },
      "outputs": [],
      "source": [
        "# Basic tensor operations for materials data\n",
        "print(\"üîß Tensor Operations for Materials Science\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create tensors representing material properties\n",
        "# Example: Bandgaps for different materials (in eV)\n",
        "bandgaps = torch.tensor([1.12, 1.42, 1.8, 1.6, 0.0, 5.9], dtype=torch.float32)\n",
        "materials = [\"Si\", \"GaAs\", \"MoS2\", \"WSe2\", \"Graphene\", \"hBN\"]\n",
        "\n",
        "print(f\"Material bandgaps (eV): {bandgaps}\")\n",
        "print(f\"Materials: {materials}\")\n",
        "print(f\"Tensor shape: {bandgaps.shape}\")\n",
        "print(f\"Data type: {bandgaps.dtype}\")\n",
        "\n",
        "# Statistical operations useful for materials analysis\n",
        "print(f\"\\nüìä Statistical Analysis:\")\n",
        "print(f\"Mean bandgap: {torch.mean(bandgaps):.2f} eV\")\n",
        "print(f\"Standard deviation: {torch.std(bandgaps):.2f} eV\")\n",
        "print(f\"Min/Max: {torch.min(bandgaps):.1f} / {torch.max(bandgaps):.1f} eV\")\n",
        "\n",
        "# 2D tensor: Crystal structure data (simplified)\n",
        "# Rows: materials, Columns: [lattice_a, lattice_b, lattice_c, bandgap]\n",
        "crystal_data = torch.tensor([\n",
        "    [5.431, 5.431, 5.431, 1.12],  # Silicon\n",
        "    [5.653, 5.653, 5.653, 1.42],  # GaAs\n",
        "    [3.160, 3.160, 12.30, 1.80],  # MoS2\n",
        "    [3.280, 3.280, 12.96, 1.60],  # WSe2\n",
        "], dtype=torch.float32)\n",
        "\n",
        "print(f\"\\nüî¨ Crystal Structure Data:\")\n",
        "print(f\"Shape: {crystal_data.shape} (4 materials √ó 4 properties)\")\n",
        "print(f\"Data:\\n{crystal_data}\")\n",
        "\n",
        "# Tensor indexing and slicing\n",
        "lattice_parameters = crystal_data[:, :3]  # First 3 columns (lattice params)\n",
        "bandgaps_2d = crystal_data[:, 3]          # Last column (bandgaps)\n",
        "\n",
        "print(f\"\\nüèóÔ∏è Lattice parameters:\\n{lattice_parameters}\")\n",
        "print(f\"\\n‚ö° Extracted bandgaps: {bandgaps_2d}\")\n",
        "\n",
        "# Demonstrate automatic differentiation (autograd)\n",
        "print(f\"\\nüéØ Automatic Differentiation Example\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Create tensor with gradient tracking enabled\n",
        "lattice_a = torch.tensor([5.0], requires_grad=True)\n",
        "\n",
        "# Simple model: volume depends on lattice parameter\n",
        "# V = a¬≥ (cubic crystal)\n",
        "volume = lattice_a ** 3\n",
        "\n",
        "print(f\"Lattice parameter a = {lattice_a.item():.2f} √Ö\")\n",
        "print(f\"Volume V = a¬≥ = {volume.item():.2f} ≈≥\")\n",
        "\n",
        "# Compute gradient dV/da = 3a¬≤\n",
        "volume.backward()\n",
        "gradient = lattice_a.grad\n",
        "\n",
        "print(f\"Gradient dV/da = {gradient.item():.2f}\")\n",
        "print(f\"Analytical: dV/da = 3a¬≤ = 3 √ó {lattice_a.item()}¬≤ = {3 * lattice_a.item()**2:.2f}\")\n",
        "print(\"‚úÖ Automatic differentiation matches analytical result!\")\n",
        "\n",
        "# Visualization of tensor operations\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Bandgap distribution\n",
        "ax1.bar(materials, bandgaps.numpy(), color='steelblue', alpha=0.7)\n",
        "ax1.set_ylabel('Bandgap (eV)')\n",
        "ax1.set_title('Materials Bandgap Distribution')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Lattice parameter vs bandgap correlation\n",
        "lattice_avg = torch.mean(lattice_parameters, dim=1)  # Average lattice parameter\n",
        "ax2.scatter(lattice_avg.numpy(), bandgaps_2d.numpy(), \n",
        "           s=100, c='red', alpha=0.7, edgecolors='black')\n",
        "ax2.set_xlabel('Average Lattice Parameter (√Ö)')\n",
        "ax2.set_ylabel('Bandgap (eV)')\n",
        "ax2.set_title('Structure-Property Relationship')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Add material labels\n",
        "for i, material in enumerate([\"Si\", \"GaAs\", \"MoS2\", \"WSe2\"]):\n",
        "    ax2.annotate(material, (lattice_avg[i], bandgaps_2d[i]), \n",
        "                xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Key Learning Points:\")\n",
        "print(\"‚Ä¢ Tensors efficiently represent materials data\")\n",
        "print(\"‚Ä¢ Autograd enables gradient-based optimization\")\n",
        "print(\"‚Ä¢ Tensor operations reveal structure-property relationships\")\n",
        "print(\"‚Ä¢ This foundation enables neural network training for materials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concept_2_theory"
      },
      "source": [
        "# ‚úÖ 2. Neural Networks - Basic Architectures for Materials\n",
        "\n",
        "Neural networks are powerful function approximators that can learn complex structure-property relationships in materials. Understanding basic architectures is crucial for applying deep learning to materials informatics.\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "A feedforward neural network with one hidden layer:\n",
        "\n",
        "$\\mathbf{h} = \\sigma(\\mathbf{W_1} \\mathbf{x} + \\mathbf{b_1})$\n",
        "\n",
        "$\\mathbf{y} = \\mathbf{W_2} \\mathbf{h} + \\mathbf{b_2}$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{x}$: Input features (material descriptors)\n",
        "- $\\mathbf{W_1}, \\mathbf{b_1}$: Hidden layer weights and biases\n",
        "- $\\sigma$: Activation function (ReLU, sigmoid, tanh)\n",
        "- $\\mathbf{y}$: Output prediction (material property)\n",
        "\n",
        "**Loss function** for regression:\n",
        "$L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$ (Mean Squared Error)\n",
        "\n",
        "## Applications in Materials Science\n",
        "\n",
        "Neural networks can predict:\n",
        "- **Electronic properties**: Bandgaps, work functions, DOS\n",
        "- **Mechanical properties**: Bulk modulus, hardness, elasticity\n",
        "- **Thermodynamic properties**: Formation energies, phase stability\n",
        "\n",
        "This section demonstrates building neural networks for materials property prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "concept_2_code"
      },
      "outputs": [],
      "source": [
        "# Define a simple neural network for materials property prediction\n",
        "class MaterialsNN(nn.Module):\n",
        "    \"\"\"Simple neural network for predicting material properties.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MaterialsNN, self).__init__()\n",
        "        # Define network layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)    # Input to hidden\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)   # Hidden layer\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)   # Hidden to output\n",
        "        self.dropout = nn.Dropout(0.1)                   # Regularization\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        x = F.relu(self.fc1(x))      # First hidden layer with ReLU activation\n",
        "        x = self.dropout(x)          # Apply dropout for regularization\n",
        "        x = F.relu(self.fc2(x))      # Second hidden layer\n",
        "        x = self.fc3(x)              # Output layer (no activation for regression)\n",
        "        return x\n",
        "\n",
        "# Create synthetic materials dataset for demonstration\n",
        "def generate_materials_dataset(n_samples=1000):\n",
        "    \"\"\"Generate synthetic materials dataset for neural network training.\"\"\"\n",
        "    \n",
        "    # Features: [atomic_number_avg, lattice_parameter, density, n_electrons]\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    atomic_number = np.random.uniform(10, 80, n_samples)  # Average atomic number\n",
        "    lattice_param = np.random.uniform(3.0, 8.0, n_samples)  # Lattice parameter (√Ö)\n",
        "    density = np.random.uniform(2.0, 15.0, n_samples)    # Density (g/cm¬≥)\n",
        "    n_electrons = np.random.uniform(10, 100, n_samples)   # Number of valence electrons\n",
        "    \n",
        "    # Stack features\n",
        "    X = np.column_stack([atomic_number, lattice_param, density, n_electrons])\n",
        "    \n",
        "    # Target: Synthetic bandgap with realistic trends\n",
        "    # Bandgap tends to decrease with atomic number, increase with lattice parameter\n",
        "    bandgap = (5.0 - 0.03 * atomic_number + \n",
        "               0.2 * lattice_param - \n",
        "               0.05 * density + \n",
        "               0.01 * n_electrons + \n",
        "               np.random.normal(0, 0.3, n_samples))  # Add noise\n",
        "    \n",
        "    # Ensure realistic bandgap range (0 to 6 eV)\n",
        "    bandgap = np.clip(bandgap, 0, 6)\n",
        "    \n",
        "    return X, bandgap\n",
        "\n",
        "# Generate training data\n",
        "print(\"üî¨ Generating Synthetic Materials Dataset\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "X, y = generate_materials_dataset(n_samples=1000)\n",
        "feature_names = ['Atomic Number', 'Lattice Param (√Ö)', 'Density (g/cm¬≥)', 'N Electrons']\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Features: {feature_names}\")\n",
        "print(f\"Target: Bandgap (eV)\")\n",
        "print(f\"Bandgap range: {y.min():.2f} - {y.max():.2f} eV\")\n",
        "\n",
        "# Show sample data\n",
        "df_sample = pd.DataFrame(X[:5], columns=feature_names)\n",
        "df_sample['Bandgap (eV)'] = y[:5]\n",
        "print(f\"\\nüìä Sample Data:\")\n",
        "print(df_sample.round(2))\n",
        "\n",
        "# Split data and prepare for PyTorch\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features (important for neural networks)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train)\n",
        "y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "print(f\"\\nüéØ Training set: {X_train_tensor.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_tensor.shape[0]} samples\")\n",
        "\n",
        "# Initialize neural network\n",
        "input_size = X.shape[1]  # Number of features\n",
        "hidden_size = 64         # Hidden layer size\n",
        "output_size = 1          # Predict single value (bandgap)\n",
        "\n",
        "model = MaterialsNN(input_size, hidden_size, output_size)\n",
        "print(f\"\\nüß† Neural Network Architecture:\")\n",
        "print(f\"Input size: {input_size} features\")\n",
        "print(f\"Hidden layers: 2 √ó {hidden_size} neurons\")\n",
        "print(f\"Output size: {output_size} (bandgap prediction)\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Training Setup:\")\n",
        "print(f\"Loss function: Mean Squared Error\")\n",
        "print(f\"Optimizer: Adam (lr=0.001)\")\n",
        "print(f\"Activation: ReLU\")\n",
        "print(f\"Regularization: Dropout (10%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "concept_2_training"
      },
      "outputs": [],
      "source": [
        "# Train the neural network\n",
        "print(\"üöÄ Training Neural Network for Materials Property Prediction\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 200\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    optimizer.zero_grad()                    # Clear gradients\n",
        "    outputs = model(X_train_tensor)          # Predict bandgaps\n",
        "    loss = criterion(outputs.squeeze(), y_train_tensor)  # Compute loss\n",
        "    \n",
        "    # Backward pass\n",
        "    loss.backward()                          # Compute gradients\n",
        "    optimizer.step()                         # Update weights\n",
        "    \n",
        "    # Validation loss\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_test_tensor)\n",
        "        val_loss = criterion(val_outputs.squeeze(), y_test_tensor)\n",
        "    model.train()\n",
        "    \n",
        "    # Store losses\n",
        "    train_losses.append(loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "print(\"‚úÖ Training completed!\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Training predictions\n",
        "    train_pred = model(X_train_tensor).squeeze().numpy()\n",
        "    train_mae = mean_absolute_error(y_train, train_pred)\n",
        "    train_r2 = r2_score(y_train, train_pred)\n",
        "    \n",
        "    # Test predictions\n",
        "    test_pred = model(X_test_tensor).squeeze().numpy()\n",
        "    test_mae = mean_absolute_error(y_test, test_pred)\n",
        "    test_r2 = r2_score(y_test, test_pred)\n",
        "\n",
        "print(f\"\\nüìä Model Performance:\")\n",
        "print(f\"Training - MAE: {train_mae:.3f} eV, R¬≤: {train_r2:.3f}\")\n",
        "print(f\"Test - MAE: {test_mae:.3f} eV, R¬≤: {test_r2:.3f}\")\n",
        "\n",
        "# Visualization of training progress and results\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Plot 1: Training curves\n",
        "ax1.plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
        "ax1.plot(val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Mean Squared Error')\n",
        "ax1.set_title('Neural Network Training Progress')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Training predictions\n",
        "ax2.scatter(y_train, train_pred, alpha=0.5, s=20, color='blue', label='Training')\n",
        "ax2.plot([0, 6], [0, 6], 'r--', label='Perfect Prediction')\n",
        "ax2.set_xlabel('True Bandgap (eV)')\n",
        "ax2.set_ylabel('Predicted Bandgap (eV)')\n",
        "ax2.set_title(f'Training Predictions (R¬≤ = {train_r2:.3f})')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Test predictions\n",
        "ax3.scatter(y_test, test_pred, alpha=0.6, s=20, color='red', label='Test')\n",
        "ax3.plot([0, 6], [0, 6], 'r--', label='Perfect Prediction')\n",
        "ax3.set_xlabel('True Bandgap (eV)')\n",
        "ax3.set_ylabel('Predicted Bandgap (eV)')\n",
        "ax3.set_title(f'Test Predictions (R¬≤ = {test_r2:.3f})')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Residuals analysis\n",
        "test_residuals = y_test - test_pred\n",
        "ax4.scatter(test_pred, test_residuals, alpha=0.6, s=20, color='green')\n",
        "ax4.axhline(y=0, color='red', linestyle='--')\n",
        "ax4.set_xlabel('Predicted Bandgap (eV)')\n",
        "ax4.set_ylabel('Residuals (eV)')\n",
        "ax4.set_title('Residuals Analysis (Test Set)')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Neural Network Insights:\")\n",
        "print(f\"‚Ä¢ The network learned structure-property relationships from {len(X_train)} training examples\")\n",
        "print(f\"‚Ä¢ Test R¬≤ of {test_r2:.3f} shows good generalization to unseen materials\")\n",
        "print(f\"‚Ä¢ Average prediction error of {test_mae:.3f} eV is reasonable for materials screening\")\n",
        "print(f\"‚Ä¢ The model can now predict bandgaps for new material compositions\")"
      ]
    }
  ]
}